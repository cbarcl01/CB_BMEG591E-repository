---
title: 'Assignment 7: PGS'
output:
  github_document:
    toc: true
    toc_depth: 4
---

# Assignment Overview

In this assignment we will learn about population stratification, imputation of genotypes, and using polygenic scores.  Polygenic scores (PGSs) can be useful for predicting disease susceptibility. In order to calculate PGSs, we need two things: GWAS summary statistics (including effect sizes), and genotypes. Most of the time, only a subset of a person's genotypes are actually measured (e.g. via SNP array), and so we must impute the rest using a matched population of fully genotyped individuals. This is the goal of Assignment 7.

Throughout the assignment we will be using a Mini Cohort that has genetic data and some phenotypic variables, together with the 1000 Genomes Project samples. Both datasets are in bfile plink format, which encompasses 3 files: *.bim, .bed and .fam* all the files can be located under the following path: */usr/local/share/data/assignment_7/*


# Getting Ready 

In this assignment, we will be using the plink tool extensively. A plink tutorial can be found here: https://zzz.bwh.harvard.edu/plink/tutorial.shtml

```{bash, eval=FALSE}

## Install plink1.9 onto your A1 conda environment:

conda activate Gnme_Assignment_1
conda install -n Gnme_Assignment_1 -c bioconda plink

```


# Genotyping Quality Control


## General QC

Before we can start working on the genetic data, we need to ensure that the quality is adequate. Thus, we are gonna check the following measuring for our MiniCohort:

   1. **SNP call rate:** The call rate represents the percentage of participants with non-missing data for that SNP. Removing variants with a call rate lower than 95% avoids potential wrong calls to be included in further analysis
   
   2. **Minor Allele Frequency:** The minor allele frequency (MAF) echoes the less common allele frequency across the population. The MAF estimates tend to be more accurate for higher MAFs and the population sample size the MAF was based on. If there are too few samples representing the rare-allele, is hard to distinguish between a true rare-allele and sequencing errors.
   
   3. **Sample call rate:** Similar to SNP call rate, it allows to filter out all samples exceeding 98% missing genetic variants out of all the  calls. 
   

```{bash, eval=FALSE}
## Using only one run of plink 1.9 (with different flags)
## 1. Filter out -SNPs- with more than 5% missingness
## 2. Filter out -variants- with less than 1% MAF
## 3. Filter out -samples- with more than 2% missingness
## 4. Create an output file in bfile format (which contains the bed, fam and bim files) for the MiniCohort QCed data

#?# Type the command you used below: - 3pt

plink --bfile /usr/local/share/data/assignment_7/Mini_cohort --mind 0.02 --maf 0.01 --geno 0.05 --make-bed --out ./Mini_CohortQC

```

**Output** 

251658 variants loaded from .bim file.</br>
2504 people (1233 males, 1271 females) loaded from .fam.</br>
0 people removed due to missing genotype data (--mind).</br>
Using 1 thread (no multithreaded calculations invoked).</br>
Before main variant filters, 2497 founders and 7 nonfounders present.</br>
Calculating allele frequencies... done.</br>
0 variants removed due to missing genotype data (--geno).</br>
623 variants removed due to minor allele threshold(s)
(--maf/--max-maf/--mac/--max-mac).</br>
251035 variants and 2504 people pass filters and QC.</br>
Note: No phenotypes present.</br>


## Global Ancestry Investigation

In order to enhance imputation accuracy when dealing with ethnically diverse cohorts is important to understand the genetic ancestries of the cohort's participants. Knowing the ancestral populations will ensure that the most closely related population is used as a reference for the imputation. For instance, one would not want to impute haplotypes of an individual of Yoruban ancestry with a population of East Asians because many of the haplotypes will differ between the two ancestries, leading to imputing the wrong variants for the Yoruban person. Hence, we will analyze the global ancestry of our cohort using Principal Component Analysis (PCA). PCA is an unsupervised, unbiased way to reduce the complexity of multidimensional.

## a. PCA-specific QC

We first need to ensure that only the most informative genetic variants are used in the analysis. To do this, we will: 

   1. **Filter out high linkage disequilibrium (LD) regions:** Because high LD regions will add redundancy to the PCA (leading to these regions dominating top PCs), they need to be removed. 
   
   2. **LD pruning:** Similarly, LD causes redundancy even outside the particularly problematic high-LD regions. Thus, we will use LD-pruning to identify variants that are in LD, and select one per block.
   
```{bash, eval=FALSE}

## Using only one run of plink 1.9 (with different flags)
## 1. Filter out the high-LD regions contained in the --high_LD_regions_hg19.txt-- file, located in /usr/local/share/data/assignment_7/
## 2. Use the --indep-pairwise to do LD prunning with the following parameters:
## - Window size: 200, 
## - Variant Count: 100 
## - VIF (variance inflation factor): 0.2 
#?# Type the command you use to create the Mini Cohort PCA-QCed bfile below: - 1pt

plink --bfile ./Mini_CohortQC  --remove /usr/local/share/data/assignment_7/high_LD_regions_hg19.txt --indep-pairwise 200kb 100 0.2 --make-bed --out ./Mini_Cohort_LD_QC

## Use the output -.prune.in- file to extract only the informative variants and create a new bfile format (bed, fam and bim files) from:
## 1. The General MiniCohort QC bfile created before
## 2. The 1KGP_reference bfile located in /usr.local/share/data/assignment_7/

#?# Type the commands you used below: - 3pt

plink --bfile ./Mini_Cohort_LD_QC --extract ./Mini_Cohort_LD_QC.prune.in --make-bed --out ./Mini_Cohort_Pruned

plink --bfile /usr/local/share/data/assignment_7/1KGP_reference --make-bed --out ./1KGP_reference

```

## b. PCA computation

To assess the ethnic diversity in our cohort, we will use One-thousand Genome Project (1KGP) data as a reference for our PCA analysis. These dataset has genetic information of major continental populations: Admixed American (AMR), European (EU), Asian (AS) and African (A). 

```{bash, eval=FALSE}

## Merge your pruned bfiles of the Mini_cohort and the 1KGP created on the previous step 
## Remember to create a new bfile (.fam, .bed and .bim files) that contains the merged data.
## IMPORTANT TIME CONSTRAINT: This step can take ~15 minutes, so make sure to check the server status before you run it!
#?# Type the command you used below: - 1pt

plink --bfile ./Mini_Cohort_Pruned --bmerge ./1KGP_reference --make-bed --out ./merged


#?# Perform a PCA analysis in plink on the merged set - 1 pt

plink --bfile ./merged --pca --out ./mergePCA

```



15831 MB RAM detected; reserving 7915 MB for main workspace.
2504 people loaded from ./Mini_Cohort_Pruned.fam.
2504 people to be merged from ./1KGP_reference.fam.
Of these, 0 are new, while 2504 are present in the base dataset.
219988 markers loaded from ./Mini_Cohort_Pruned.bim.
251658 markers to be merged from ./1KGP_reference.bim.
Of these, 31670 are new, while 219988 are present in the base dataset.
Performing single-pass merge (2504 people, 251658 variants).

## c. Visualization

```{r, eval = FALSE}

## Copy the PCA .eigenvec file to your computer, together with the samples_info.txt located in /usr/local/share/data/assignment_7/

pscp -P 22 cbarcl01@gi-edu-sv4.bme.ubc.ca:/home/cbarcl01/Assignment_7/mergePCA.eigenvec C:\Users\cbarc\OneDrive\Desktop

pscp -P 22 cbarcl01@gi-edu-sv4.bme.ubc.ca:/usr/local/share/data/assignment_7/samples_info.txt C:\Users\cbarc\OneDrive\Desktop
```

```{r}
## Load the .eigenvec file onto R, change the column names to: FID, IID, PC1, PC2, PC3, ..., PC20
#?# Type the command you used below: - 1pt

library(tidyverse)
eigenvec <-read.table("C:/Users/cbarc/OneDrive/Desktop/git_temp/CB_BMEG591E-repository/Assignment_7/mergePCA.eigenvec")

PCA <- as_tibble(eigenvec)

PCA <- rename(PCA, FID = V1, IID = V2, PC1 = V3, PC2 = V4, PC3 = V5, PC4 = V6, PC5 = V7, PC6 = V8, PC7 = V9, PC8 = V10, PC9 = V11, PC10 = V12, PC11 = V13, PC12 = V14, PC13 = V15, PC14 = V16, PC15 = V17, PC16 = V18, PC17 = V19, PC18 = V20, PC19 = V21, PC20 = V22)
                      
head(PCA)

## Load the samples_info.txt file onto R, change the column names to: FID, IID, SuperPopulation, Population
#?# Tyoe the commands you used below: - 1pt
 
 
sampleInfo <- read.table("C:/Users/cbarc/OneDrive/Desktop/git_temp/CB_BMEG591E-repository/Assignment_7/samples_info.txt")
sampleInfo <- as_tibble(sampleInfo)

sampleInfo <- rename(sampleInfo, IID = V1, FID = V2, SuperPopulation = V3, Population = V4) 

sampleInfo %>% relocate(IID, .after = FID)




## Merge the .eigenvec and sample_info data.frames together using the IID column
## Tip: Look into the -merge- function!
#?# Type the command you used below: - 1pt

mergedData <- merge(PCA, sampleInfo, by="IID")
```

```{r}

## Using ggplot create a scatterplot, using: 
## x-axis: PC1
## y-axis: PC2
## color: SuperPopulation - to use the Population information to color the samples and be able to appreciate population structure!
#?# Type the command you used below: 1pt

library(ggplot2)

ggplot(mergedData, aes(x = PC1 , y = PC2, colour = SuperPopulation )) + 
   geom_point()
```
 
#?# Where do the cohort samples fall? Are they all clustered together? - 1 pt

Our cohort samples are not as tightly clustered as some groups (for instance the European population). The data points overlap with both AMR - Admixed American, and SAS - South Asian.


#?# Which Population would you use as a reference for imputation?, Why? - 1 pt

I would use the AMR as a reference for imputation, because although the points overlap with the SAS popultaion as well, the points extend outside of the SAS cluster. Our cohort data points are completely within the AMR cluster. However, there is still large variance and overlap between populations which makes this decision difficult.

#?# Do you think looking at the top two PCs is sufficient to tell what population is best? Why/why not? - 2 pt

No. There is still a lot of overlap between the clusters which it's difficult to determine the best Population for imputation and comparison.


# Imputation

Imputation of genetic data is a very computationally intensive analysis, that can take a long time. So we have performed it for you. Using the chromosome 17 imputation information located in */usr/local/share/data/assignment_7/* under the *Mini_cohort_chr17_imputation_results.info.gz* we will calculate some post-imputation metrics. 

```{r, eval = FALSE}
## Load the Mini_cohort_chr17_imputation_results.info.gz file to your Rstudio environment 
### Copy to local machine

pscp -P 22 cbarcl01@gi-edu-sv4.bme.ubc.ca:/usr/local/share/data/assignment_7/Mini_cohort_chr17_imputation_results.info C:\Users\cbarc\OneDrive\Desktop

```

```{r}
## Load the Mini_cohort_chr17_imputation_results.info.gz file to your Rstudio environment 
chr17 <-read.table("C:/Users/cbarc/OneDrive/Desktop/git_temp/CB_BMEG591E-repository/Assignment_7/Mini_cohort_chr17_imputation_results.info.gz", header = TRUE)

chr17 <- as_tibble(chr17)

##Use the information in the file to answer the following questions. Accompany each of the answers with the code you used to get to them and a brief explanation of your thought process behind.

#?# What is the percentage of imputed SNPs? 0.5 pt

Impute <- chr17 %>%
   filter (Genotyped == "Imputed") #filtering on value imputed to identify all records imputed

Impute <- count(Impute) #saving as a new variable the number of records where 'imputed'

Genotype <- chr17 %>%
   filter (Genotyped == "Genotyped")  #filtering on value imputed to identify all records genotyped

Genotype <- count(Genotype) #saving as a new variable the number of records where 'genotyped'


Total <- count(chr17) #counting rows in dataset to get total


PerImputed <- (Impute / Total)*100 ## calculates new variable 'Per' where Imputed SNPs number is divided by the total and multiplied by 100.
   
```

#?# What is the percentage of imputed SNPs? 0.5 pt

Imputed SNPs is str(PerImputed)


```{r}
## The metric of imputation quality is Rsq, this is the estimated value of the squared correlation between imputed and true genotypes. Since true genotypes are not available, this calculation is based on the idea that poorly imputed genotype counts will shrink towards their expectations based on allele frequencies observed in the population (https://genome.sph.umich.edu/wiki/Minimac3_Info_File#Rsq).  An Rsq < 0.3 is often used to flag poorly imputed SNPs. 

#?# What is the percentage of poorly imputed SNPs?


PoorImpute <- chr17 %>%
   filter (Genotyped == "Imputed", Rsq <= 0.3) #filtering on value imputed to identify all records poorly imputed

PoorImpute <- count(PoorImpute) #saving as a new variable the number of records where 'imputed'

PerPoorImputed <- (PoorImpute / Total)*100 ## calculates new variable 'Per' where Imputed SNPs number is divided by the total and multiplied by 100.
```

#?# What is the percentage of poorly imputed SNPs? 0.5 pt

The percentage of poorly imputed SNPs, ie at a value of < 0.3 is str(PerPoorImputed)


```{r}
#?# Create a histogram to visualize the distribution of the MAF - 1 pt

ggplot(chr17, aes(x=MAF)) +
   geom_histogram(bins =20)

```




#?# Which MAF is most frequent? What does that mean? - 1 pt


#?# What is the maximum MAF? Why is that? - 1 pt

From the histogram distribution we can see the maximum MAF is 0.5. MAF is the minor allele frequency. As the 4 bases pair together, this means there is always only 1 of 2 options and therefore the maximum MAF is 0.5. If the MAF was > than 0.5 this would be the common allele and the remainder would be the MAF value.


# Polygenic Scores (PGS)  

A GWAS for affinity for tapas (the Spanish appetizer) was performed and 199 SNPs were found significantly associated. The significant SNPs and their assigned effect sizes are described in the *Tapas_enjoyability_GWAS_sumStats.txt* file. Thanks to the imputation performed in our MiniCohort, we were able to obtain the dosages (double risk alleles=2, one risk allele=1, no risk alleles=0) for each one of the SNPs associated to the Tapas 'enjoyability', described in the *MiniCohort_Tapas_SNPdosages.txt*. 

PGS are calculated by multiplying the effect sizes of each SNP by the dosage of an individual for those SNP and then adding together all the effectSize x dosage. The formula is outlined below, where:

  - i: individual of which you are calculating the PGS
  
  - j: SNP that has been found to be associated to the trait (Tapas enjoyability in this case)

  - Beta: Effect size

  - dosage: number of risk alleles the *individual i* has of the *risk allele j*? (2,1 or 0)

![](PGS_formula.png)

```{r}

## Load to your RStudio:
## 1.  -Tapas_enjoyability_GWAS_sumStats.txt-
## 2.  -MiniCohort_Tapas_SNPdosages.txt- 
## Both are located in the A7 directory on github.

TapasE <-read.table("C:/Users/cbarc/OneDrive/Desktop/git_temp/CB_BMEG591E-repository/Assignment_7/Tapas_enjoyability_GWAS_sumStats.txt", header = TRUE)

TapasE <- as_tibble(TapasE)

TapasSNP <-read.table("C:/Users/cbarc/OneDrive/Desktop/git_temp/CB_BMEG591E-repository/Assignment_7/MiniCohort_Tapas_SNPdosages.txt", header = TRUE)

TapasSNP <- as_tibble(TapasSNP)

## Using the base PRS formula outlined below, calculate the Tapas enjoyability PGS for the individuals in the Mini Cohort 
#?# Include your rationale and the documented code you used - 5pt

#check that the SNPs are in the same order in both data sets, if they are we can use matrix multiplication to calculate the PGS for individuals.

x <- TapasSNP %>% 
   select(rs58108140:rs188144421)

identical(colnames(x) , TapasE$SNP)

#They are identical so we can now conduct matrix multiplication.
y <- TapasE %>%
   select(Effect_Size)

x <- as.matrix(x)
y <- as.matrix(y)
PGS <- x %*% y
PGS <- as.data.frame(PGS)

#?# Use ggplot to plot the distribution of the Tapas PGS: - 2 pt
## Include the code and the graph in your analysis! 
## Tip: http://www.cookbook-r.com/Graphs/Plotting_distributions_(ggplot2)/

ggplot(PGS, aes(x=Effect_Size)) + geom_density()

```

#?# What is the distribution of the tapas PGS? - 1pt

This is displaying a Bimodal distribution. Data look to be normally distributed, but with 2 peaks. This often implies that our data include two different normally distributed groups. 

## PGS accuracy

```{r}
## The Tapas enjoyability was measured in a range of 0-1, with 0 being hating tapas and 1 being completely in love with tapas.
## This tapas likability is captured in the "Tapas_enjoyability" column of the -MiniCohort_Tapas_SNPdosages.txt- file.

#?# Make a scatterplot with a linear regression line, where x is the Tapas-PGS and y is their actual Tapas enjoyability - 2 pt
## Tip: http://www.sthda.com/english/wiki/ggplot2-scatter-plots-quick-start-guide-r-software-and-data-visualization

z<- PGS %>% mutate(SNP = TapasSNP$IID, Tapas_enjoyability = TapasSNP$Tapas_enjoyability)

ggplot(z, aes(x=Effect_Size, y=Tapas_enjoyability)) + 
  geom_point()+
  geom_smooth(method=lm)
```

```{r}
#?# What is the correlation coeficient between the PGS and Tapas enjoyability? Is Spearman or Pearson correlation more appropriate here? Why? - 3 pt

cor(z$Effect_Size, z$Tapas_enjoyability, method ="pearson")
cor(z$Effect_Size, z$Tapas_enjoyability, method ="spearman")

```

In this case Spearman rank correlation appears to be the best option. As the variable Tapas_enjoyability is between 0 and 1 and is rounded to 1 decimal place it becomes essentially ordinal. Spearman rank works with ordinal data, where Pearson does not.

#?# How predictive is the PGS for tapas preference? Include in your answer why do you think it is/isn't accurate and what could affect its predicitvity - 2pt 

I do not believe PGS is very predicitve of tapas preference. The correlation coefficient is close to 0.


